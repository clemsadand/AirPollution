{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/clemsadand/AirPollution.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:12.489099Z","iopub.execute_input":"2025-04-29T14:28:12.489294Z","iopub.status.idle":"2025-04-29T14:28:13.254009Z","shell.execute_reply.started":"2025-04-29T14:28:12.489276Z","shell.execute_reply":"2025-04-29T14:28:13.253059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd AirPollution","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:13.256132Z","iopub.execute_input":"2025-04-29T14:28:13.256405Z","iopub.status.idle":"2025-04-29T14:28:13.262818Z","shell.execute_reply.started":"2025-04-29T14:28:13.256384Z","shell.execute_reply":"2025-04-29T14:28:13.262063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !apt-get update -q\n# !apt-get install -y libglu1-mesa -q\n# !pip install gmsh numpy matplotlib meshio -q\n# !pip install pyDOE -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:13.263722Z","iopub.execute_input":"2025-04-29T14:28:13.264312Z","iopub.status.idle":"2025-04-29T14:28:13.278639Z","shell.execute_reply.started":"2025-04-29T14:28:13.264291Z","shell.execute_reply":"2025-04-29T14:28:13.277881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt-get update -q\n!sudo apt-get install -y libglu1-mesa -q\n!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:13.279532Z","iopub.execute_input":"2025-04-29T14:28:13.279760Z","iopub.status.idle":"2025-04-29T14:28:32.679181Z","shell.execute_reply.started":"2025-04-29T14:28:13.279739Z","shell.execute_reply":"2025-04-29T14:28:32.678468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:32.680093Z","iopub.execute_input":"2025-04-29T14:28:32.680306Z","iopub.status.idle":"2025-04-29T14:28:32.684288Z","shell.execute_reply.started":"2025-04-29T14:28:32.680285Z","shell.execute_reply":"2025-04-29T14:28:32.683557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# n_dofs = [45, 191, 846, 3431, 13998, 56408, 226206]\n# for i, n_ in enumerate(n_dofs):\n#     if i<=5:\n#         layers = [3] + [32] * (1+i) + [1]\n#     else:\n#         layers = [3] + [60] * (i) + [1]\n#     n_params = sum(l1 * l2 + l2 for l1, l2 in zip(layers[:-1], layers[1:]))\n#     print(f\"{n_} --:-- {n_params}\")\n#     # print((n_dofs[i] - n_params)/n_dofs[i])\n#     # print((n_-1)/5)\n#     print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:32.686666Z","iopub.execute_input":"2025-04-29T14:28:32.687016Z","iopub.status.idle":"2025-04-29T14:28:32.701210Z","shell.execute_reply.started":"2025-04-29T14:28:32.687000Z","shell.execute_reply":"2025-04-29T14:28:32.700538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef twolayers(w):\n    return w**2 + 6*w+1\n\ndef threelayers(w):\n    return 2*w**2 + 7*w+1\n\ndef fourlayers(w):\n    return 3*w**2 + 8*w + 1\n\ndef cost(w, n_dofs, ratio, loss_fn):\n    return np.abs(loss_fn(w) / n_dofs - ratio)\n\nn_dofs_vals = [45, 191, 846, 3431, 13998, 56408]\nratio = 0.09\n\nw_vals = np.arange(2, 1000)\nfor n_dofs in n_dofs_vals:\n    loss_fn = fourlayers\n    cost_vals = cost(w_vals, n_dofs, ratio, loss_fn)\n\n    max_cost = np.min(cost_vals)\n    smallest_w_for_max = w_vals[np.argmin(cost_vals)]\n\n    print(f\"Max cost: {max_cost}\")\n    print(f\"Smallest w for max cost: {smallest_w_for_max}\")\n    print(f\"Number of params: {loss_fn(smallest_w_for_max)}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:32.701827Z","iopub.execute_input":"2025-04-29T14:28:32.702067Z","iopub.status.idle":"2025-04-29T14:28:32.716412Z","shell.execute_reply.started":"2025-04-29T14:28:32.702046Z","shell.execute_reply":"2025-04-29T14:28:32.715855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fourlayers(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:32.717432Z","iopub.execute_input":"2025-04-29T14:28:32.717728Z","iopub.status.idle":"2025-04-29T14:28:32.730530Z","shell.execute_reply.started":"2025-04-29T14:28:32.717698Z","shell.execute_reply":"2025-04-29T14:28:32.729978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[45, 191, 846, 3431, 13998, 56408]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:32.731206Z","iopub.execute_input":"2025-04-29T14:28:32.731398Z","iopub.status.idle":"2025-04-29T14:28:32.743425Z","shell.execute_reply.started":"2025-04-29T14:28:32.731384Z","shell.execute_reply":"2025-04-29T14:28:32.742764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_neurons = [2, 4, 8, 16, 32, 64]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:32.744418Z","iopub.execute_input":"2025-04-29T14:28:32.744786Z","iopub.status.idle":"2025-04-29T14:28:32.757609Z","shell.execute_reply.started":"2025-04-29T14:28:32.744768Z","shell.execute_reply":"2025-04-29T14:28:32.756932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Imports ---\nimport numpy as np\nimport crbe_fem\nimport pinn2\nimport meshio\nfrom tqdm import tqdm\nimport time\nimport torch\nimport psutil\nimport pandas as pd\n\n# --- Function to track GPU and CPU memory ---\ndef get_gpu_memory():\n    if torch.cuda.is_available():\n        return torch.cuda.max_memory_allocated() / 1e6  # in MB\n    return 0\n\ndef get_cpu_memory():\n    return psutil.Process().memory_info().rss / 1e6  # in MB\n\n# --- Problem Setup ---\ndomain_size = 20.0\nLx = Ly = domain_size\nT = 10.0\nD = 0.1\nvx, vy = 1.0, 0.5\nsigma = 0.1\n\ndomain_params = crbe_fem.DomainParams(Lx=Lx, Ly=Ly, T=T)\nmodel_params = crbe_fem.Models(vx=vx, vy=vy, D=D, sigma=sigma)\n\nt_domain = [0.0, T]\nx_domain = [-domain_size, domain_size]\ny_domain = [-domain_size, domain_size]\nlb = np.array([t_domain[0], x_domain[0], y_domain[0]])\nub = np.array([t_domain[1], x_domain[1], y_domain[1]])\n\ndomain = [-20, 20, -20, 20]\ntime_range = [0, 10]\nv = [vx, vy]\n\nlambda_weights = {'pde': 1.0, 'ic': 10.0, 'bc': 10.0}\nlearning_rate = 0.001\nepochs = 20000\nn_steps = 128\nT_final = T\n\n# --- Experimental Settings ---\nmesh_sizes = [4, 8, 16, 32, 64, 128]\nn_neurons = [2, 4, 8, 16, 32, 64]\n\n# --- Logging ---\nn_dofs = []\nn_boundary_dofs = []\npinn_results = []\nresult_history = {}\n\n# --- Main Loop ---\nfor i, mesh_size in enumerate(mesh_sizes):\n    # Reset GPU memory tracking\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.empty_cache()\n    initial_cpu_memory = get_cpu_memory()\n    initial_gpu_memory = get_gpu_memory()\n\n    layers = [3] + [n_neurons[i]] * 4 + [1]\n    mesh_file = crbe_fem.create_mesh(mesh_size, domain_size=domain_size)\n    mesh = meshio.read(mesh_file)\n    mesh_data = crbe_fem.MeshData(mesh, domain_params, nt=n_steps)\n\n    n_dofs.append(mesh_data.number_of_segments)\n    n_boundary_dofs.append(len(mesh_data.boundary_segments))\n\n    n_ic = round(0.2 * mesh_data.number_of_segments)\n    n_bc = n_ic\n    n_col = mesh_data.number_of_segments - n_ic - n_bc\n    batch_sizes = {'pde': n_col, 'ic': n_ic, 'bc': n_ic}\n\n    model = pinn2.PINN(layers, D, v)\n\n    print(f\"Training for mesh size {mesh_size} ...\")\n    start_time = time.time()\n    history = pinn2.train_pinn(model, domain, time_range, batch_sizes, learning_rate, epochs, lambda_weights)\n    train_time = time.time() - start_time\n\n    final_gpu_memory = get_gpu_memory()\n    final_cpu_memory = get_cpu_memory()\n\n    result_history[f\"mesh_size_{mesh_size}\"] = history\n\n    x_eval_flat = mesh_data.midpoints[:,0].reshape(-1, 1)\n    y_eval_flat = mesh_data.midpoints[:,1].reshape(-1, 1)\n    t_eval = np.ones_like(x_eval_flat) * T_final\n\n    rel_l2_error, max_error, u_pred, u_exact = pinn2.compute_error(\n        model, t_eval, x_eval_flat, y_eval_flat\n    )\n\n    pinn_results.append({\n        \"time\": T_final,\n        \"rel_l2_error\": rel_l2_error,\n        \"max_error\": max_error,\n        \"mesh_size\": mesh_size,\n        \"train_time\": train_time,\n        \"final_loss\": history[\"total_loss\"][-1],\n        \"number_of_collocation_points\": mesh_data.number_of_segments,\n        \"n_parameters\": sum(l1 * l2 + l2 for l1, l2 in zip(layers[:-1], layers[1:])),\n        \"gpu_memory_usage_MB\": final_gpu_memory - initial_gpu_memory,\n        \"cpu_memory_usage_MB\": final_cpu_memory - initial_cpu_memory,\n    })\n\n    print(f\"Mesh size: {mesh_size}\")\n    print(f\"GPU Memory: {final_gpu_memory - initial_gpu_memory:.2f} MB\")\n    print(f\"CPU Memory: {final_cpu_memory - initial_cpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\n    del model\n\n# --- Export Results ---\ndf_pinn = pd.DataFrame(pinn_results)\ndf_pinn\n\n# # Optional: Visualize\n# import matplotlib.pyplot as plt\n# plt.figure(figsize=(12, 6))\n# plt.subplot(1, 2, 1)\n# plt.plot(df_pinn['mesh_size'], df_pinn['gpu_memory_usage_MB'], 'o-', label='GPU Memory')\n# plt.xlabel('Mesh Size'); plt.ylabel('Memory Usage (MB)')\n# plt.title('GPU Memory Usage'); plt.grid(True); plt.xscale('log')\n\n# plt.subplot(1, 2, 2)\n# plt.plot(df_pinn['mesh_size'], df_pinn['cpu_memory_usage_MB'], 'o-', label='CPU Memory')\n# plt.xlabel('Mesh Size'); plt.ylabel('Memory Usage (MB)')\n# plt.title('CPU Memory Usage'); plt.grid(True); plt.xscale('log')\n\n# plt.tight_layout()\n# plt.savefig('memory_usage.png')\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:28:32.758465Z","iopub.execute_input":"2025-04-29T14:28:32.758718Z","iopub.status.idle":"2025-04-29T14:48:58.128589Z","shell.execute_reply.started":"2025-04-29T14:28:32.758692Z","shell.execute_reply":"2025-04-29T14:48:58.127951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n\n# df_pinn = pd.DataFrame(pinn_results)\ndf_pinn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:48:58.129434Z","iopub.execute_input":"2025-04-29T14:48:58.129973Z","iopub.status.idle":"2025-04-29T14:48:58.140561Z","shell.execute_reply.started":"2025-04-29T14:48:58.129945Z","shell.execute_reply":"2025-04-29T14:48:58.139869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport crbe_fem\nimport pinns\nimport meshio\nfrom tqdm import tqdm\nimport time\nimport psutil\nimport torch\nimport pandas as pd\nimport gc\n\n# --- Problem Setup ---\ndomain_size = 20.0\nLx = Ly = domain_size\nT = 10.0\nD = 0.1\nvx, vy = 1.0, 0.5\nsigma = 0.1\n\ndomain_params = crbe_fem.DomainParams(Lx=Lx, Ly=Ly, T=T)\nmodel_params = crbe_fem.Models(vx=vx, vy=vy, D=D, sigma=sigma)\n\nt_domain = [0.0, T]\nx_domain = [-domain_size, domain_size]\ny_domain = [-domain_size, domain_size]\n\nlb = np.array([t_domain[0], x_domain[0], y_domain[0]])\nub = np.array([t_domain[1], x_domain[1], y_domain[1]])\n\nexact_solutions = {\n    \"name\": \"Impulsion gaussienne\",\n    \"func\": lambda t, x, y: pinns.exact_solution_gaussian_pulse(t, x, y, D, [vx, vy]),\n    \"epochs\": 1000,\n    \"lr\": 3e-4\n}\n\nexact_sol_name = exact_solutions[\"name\"]\nexact_sol_fn = exact_solutions[\"func\"]\nepochs = exact_solutions[\"epochs\"]\nlr = exact_solutions[\"lr\"]\n\nmesh_sizes = [4, 8, 16, 32, 64, 128]\nn_steps = 128\ncrbe_results = []\n\n# --- Function to track CPU memory ---\ndef get_cpu_memory():\n    return psutil.Process().memory_info().rss / 1e6  # in MB\n\n# --- Loop over mesh sizes ---\nfor i, mesh_size in enumerate(mesh_sizes):\n    # --- Prepare for memory tracking ---\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\n    initial_cpu_memory = get_cpu_memory()\n\n    print(f\"Training for mesh size = {mesh_size} ...\")\n    start_time = time.time()\n\n    # --- Mesh and Solver Setup ---\n    mesh_file = crbe_fem.create_mesh(mesh_size, domain_size=domain_size)\n    mesh = meshio.read(mesh_file)\n    mesh_data = crbe_fem.MeshData(mesh, domain_params, nt=n_steps)\n\n    solver = crbe_fem.BESCRFEM(domain_params, model_params, mesh_data, use_quadrature=True)\n\n    solver.solve()\n    errors = solver.compute_errors()\n    train_time = time.time() - start_time\n\n    # --- Memory tracking after solve ---\n    gc.collect()\n    final_cpu_memory = get_cpu_memory()\n    final_gpu_memory = torch.cuda.max_memory_allocated() / 1e6 if torch.cuda.is_available() else 0\n\n    # --- Save results ---\n    crbe_results.append({\n        \"solution\": exact_sol_name,\n        \"time\": T,\n        \"rel_l2_error\": errors['final_l2_error'],\n        \"max_error\": errors['final_linf_error'],\n        \"mesh_size\": mesh_size,\n        \"train_time\": train_time,\n        \"gpu_memory_usage_MB\": final_gpu_memory,\n        \"cpu_memory_usage_MB\": final_cpu_memory - initial_cpu_memory,\n        \"number_of_collocation_points\": mesh_data.number_of_segments,\n    })\n\n    # --- Print summary ---\n    print(f\"Mesh size: {mesh_size}\")\n    print(f\"Peak GPU Memory: {final_gpu_memory:.2f} MB\")\n    print(f\"CPU Memory Used: {final_cpu_memory - initial_cpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\n# --- Results as DataFrame ---\ndf_crbe = pd.DataFrame(crbe_results)\ndisplay(df_crbe)\n\n# --- Plot Memory Usage ---\nplt.figure(figsize=(10, 6))\nplt.plot(mesh_sizes, df_crbe['gpu_memory_usage_MB'], label='GPU Memory (MB)', marker='o')\nplt.plot(mesh_sizes, df_crbe['cpu_memory_usage_MB'], label='CPU Memory (MB)', marker='s')\nplt.xlabel('Mesh Size')\nplt.ylabel('Memory Usage (MB)')\nplt.title('Memory Usage vs Mesh Size')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T14:48:58.141631Z","iopub.execute_input":"2025-04-29T14:48:58.141863Z","execution_failed":"2025-04-29T14:49:46.743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pinn.to_csv(\"df_pinn_training_results.csv\")\ndf_crbe.to_csv(\"df_crbe_training_results.csv\")\n\n# import pandas as pd\n# df_pinn = pd.read_csv(\"df_pinn_training_results.csv\")\n# df_crbe = pd.read_csv(\"df_crbe_training_results.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sensitivity Analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport crbe_fem\nimport pinn2\nimport meshio\nfrom tqdm import tqdm\nimport time\n\n\n\n# Problem setup\ndomain_size = 20\ndomain = [-domain_size, domain_size, -domain_size, domain_size]  # [x_min, x_max, y_min, y_max] in km\nT_final = 10\ntime_range = [0, T_final]  # [t_min, t_max] in seconds\nv = [1.0, 0.5]  # Velocity vector in m/s\nD_list = [0.01, 0.1, 1.0]  # List of Diffusion coefficient in m²/s\nv = [1.0, 0.5]\nsigma = 0.1\n\n# Résultats\npinn_results = []\npinn_histories = {}\ncrbe_results = []\n\n# ======== FIxed parameters\nmesh_size = 256\nn_steps = 128\n\n#\ndomain_params = crbe_fem.DomainParams(Lx=domain_size, Ly=domain_size, T=T_final)\n\n\n\n#mesh\nmesh_file = crbe_fem.create_mesh(mesh_size, domain_size=domain_size)\nmesh = meshio.read(mesh_file)\nmesh_data = crbe_fem.MeshData(mesh, domain_params, nt=n_steps)\n\nn_ic = mesh_size\nn_bc = mesh_size\nn_col = mesh_data.number_of_segments - n_ic - n_bc\n\n\nlambda_weights = {'pde': 1.0, 'ic': 10.0, 'bc': 10.0}\nlearning_rate = 0.001\nbatch_sizes = {'pde': n_col, 'ic': n_ic, 'bc': n_bc}\nepochs = 10000\nlayers = [3, 20, 20, 20, 20, 20, 1]\n\nsensitivity_data = []\n\nfor i, nu in enumerate(D_list):\n    #============================================================\n    #PINN's setup\n    model = pinn2.PINN(layers, nu, v)\n\n    print(f\"Training for {mesh_size} data ...\")\n    \n    start_time = time.time()\n    history = pinn2.train_pinn(model, domain, time_range, batch_sizes, learning_rate, epochs, lambda_weights)\n    train_time = time.time() - start_time\n\n    pinn_histories[f\"mesh_size_{mesh_size}\"] = history\n\n    x_eval_flat = mesh_data.midpoints[:,0].reshape(-1, 1)\n    y_eval_flat = mesh_data.midpoints[:,1].reshape(-1, 1)\n    t_eval = np.ones_like(x_eval_flat) * T_final\n\n    rel_l2_error, max_error, u_pred, u_exact = pinn2.compute_error(\n                model, t_eval, x_eval_flat, y_eval_flat)\n\n    # # Ajout des résultats\n    # sensitivity_data.append({\n    #     \"method\": \"PINN\",\n    #     \"diffusion\": nu,\n    #     \"time\": T_final,\n    #     \"rel_l2_error\": rel_l2_error,\n    #     \"max_error\": max_error,\n    #     \"mesh_size\": mesh_size,\n    #     \"train_time\": train_time,\n    #     \"final_loss\": history[\"total_loss\"][-1],\n    #     \"number_of_collocation_points\": mesh_data.number_of_segments,\n    #     \"n_parameters\": sum(l1 * l2 + l2 for l1, l2 in zip(layers[:-1], layers[1:])),\n    # })\n    \n    #=============================================================\n    #CRBE's setup\n    exact_sol_fn = lambda t, x, y: model.f_analytical(t, x, y)\n\n    #model\n    model_params = crbe_fem.Models(vx=v[0], vy=v[1], D=nu, sigma=sigma)\n\n    print(f\"Training for {mesh_size} data ...\")\n    start_time = time.time()\n    solver = crbe_fem.BESCRFEM(domain_params, model_params, mesh_data, use_quadrature=True)\n    solver.solve()\n\n    train_time = time.time() - start_time\n    errors = solver.compute_errors()\n\n     # Ajout des résultats\n    sensitivity_data.append({\n        \"diffusion_coef\": nu,\n        \"pinn_l2_error\": rel_l2_error,\n        \"max_error\": max_error,\n        \"cr_l2_error\": errors['final_l2_error'],\n        \"cr_max_error\": errors['final_linf_error'],\n    })\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sensitivity_data = pd.DataFrame(sensitivity_data)\n# df_sensitivity_data = pd.DataFrame(sensitivity_data)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sensitivity_data","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comprehensive Dashboard","metadata":{}},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Create a figure with two subplots for L2 and Linf errors\n# plt.figure(figsize=(14, 10))\n\n# Plot 1: L2 Error\nplt.figure(figsize=(10, 6))\n\nplt.loglog(df_crbe['mesh_size'], df_crbe['rel_l2_error'], 'o-', linewidth=2, markersize=8, label='CR-BE (L2)')\nplt.loglog(df_pinn['mesh_size'], df_pinn['rel_l2_error'], 's-', linewidth=2, markersize=8, label='PINN (L2)')\n\n# Add reference lines for convergence rates\nx = np.array([4, 128])\nplt.loglog(x, 1/x, 'k--', alpha=0.7, label='O(h¹)')\nplt.loglog(x, 1/x**2, 'k-.', alpha=0.7, label='O(h²)')\n\nplt.xlabel('Mesh Size', fontsize=12)\nplt.ylabel('Relative L2 Error', fontsize=12)\nplt.title('Relative L2 Error Comparison: CR-BE vs PINN', fontsize=14)\nplt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\nplt.legend(fontsize=10)\n\nplt.tight_layout()\nplt.savefig('error_comparison_loglog_l2.png', dpi=300, bbox_inches='tight')\nplt.savefig('error_comparison_loglog_l2.pdf', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Plot 2: Linf Error\nplt.figure(figsize=(10, 6))\nplt.loglog(df_crbe['mesh_size'], df_crbe['max_error'], 'o-', linewidth=2, markersize=8, label='CR-BE (L∞)')\nplt.loglog(df_pinn['mesh_size'], df_pinn['max_error'], 's-', linewidth=2, markersize=8, label='PINN (L∞)')\n\n# Add reference lines for convergence rates\nplt.loglog(x, 1/x, 'k--', alpha=0.7, label='O(h¹)')\nplt.loglog(x, 1/x**2, 'k-.', alpha=0.7, label='O(h²)')\n\nplt.xlabel('Mesh Size', fontsize=12)\nplt.ylabel('Sup Error (L∞)', fontsize=12)\nplt.title('Sup Error (L∞) Comparison: CR-BE vs PINN', fontsize=14)\nplt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\nplt.legend(fontsize=10)\n\nplt.tight_layout()\nplt.savefig('error_comparison_loglog_linf.png', dpi=300, bbox_inches='tight')\nplt.savefig('error_comparison_loglog_linf.pdf', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Additional plot: Computational efficiency (train time vs mesh size)\nplt.figure(figsize=(10, 6))\nplt.loglog(df_crbe['mesh_size'], df_crbe['train_time'], 'o-', linewidth=2, markersize=8, label='CR-BE')\nplt.loglog(df_pinn['mesh_size'], df_pinn['train_time'], 's-', linewidth=2, markersize=8, label='PINN')\n\n# Add reference lines for computational complexity\nplt.loglog(x, x**1, 'k--', alpha=0.7, label='O(h¹)')\nplt.loglog(x, x**2, 'k-.', alpha=0.7, label='O(h²)')\n\nplt.xlabel('Mesh Size', fontsize=12)\nplt.ylabel('Training Time (s)', fontsize=12)\nplt.title('Computational Efficiency: CR-BE vs PINN', fontsize=14)\nplt.grid(True, which=\"both\", ls=\"--\", alpha=0.7) \nplt.legend(fontsize=10)\n\nplt.tight_layout()\nplt.savefig('computational_efficiency_loglog.png', dpi=300, bbox_inches='tight')\nplt.savefig('computational_efficiency_loglog.pdf', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Advanced Convergence Analysis\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import linregress\nimport seaborn as sns\nfrom matplotlib.ticker import ScalarFormatter\n\n# 1. Calculate actual convergence rates\ndef calculate_convergence_rates(df, error_column):\n    mesh_sizes = df['mesh_size'].values\n    errors = df[error_column].values\n    \n    # Calculate log values\n    log_h = np.log(1/mesh_sizes)\n    log_err = np.log(errors)\n    \n    # Linear regression to find slope (convergence rate)\n    slope, intercept, r_value, p_value, std_err = linregress(log_h, log_err)\n    \n    return slope, r_value**2\n\n# Calculate convergence rates for both methods\ncr_l2_rate, cr_l2_r2 = calculate_convergence_rates(df_crbe, 'rel_l2_error')\npinn_l2_rate, pinn_l2_r2 = calculate_convergence_rates(df_pinn, 'rel_l2_error')\ncr_linf_rate, cr_linf_r2 = calculate_convergence_rates(df_crbe, 'max_error')\npinn_linf_rate, pinn_linf_r2 = calculate_convergence_rates(df_pinn, 'max_error')\n\nprint(f\"CR-BE L2 convergence rate: {cr_l2_rate:.3f} (R² = {cr_l2_r2:.3f})\")\nprint(f\"PINN L2 convergence rate: {pinn_l2_rate:.3f} (R² = {pinn_l2_r2:.3f})\")\nprint(f\"CR-BE L∞ convergence rate: {cr_linf_rate:.3f} (R² = {cr_linf_r2:.3f})\")\nprint(f\"PINN L∞ convergence rate: {pinn_linf_rate:.3f} (R² = {pinn_linf_r2:.3f})\")\n\n# 2. Error vs computational cost (efficiency analysis)\nplt.figure(figsize=(10, 6))\nplt.loglog(df_crbe['train_time'], df_crbe['rel_l2_error'], 'o-', linewidth=2, markersize=8, label=f'CR-BE (Rate: {cr_l2_rate:.2f})')\nplt.loglog(df_pinn['train_time'], df_pinn['rel_l2_error'], 's-', linewidth=2, markersize=8, label=f'PINN (Rate: {pinn_l2_rate:.2f})')\n\nplt.xlabel('Computational Time (s)', fontsize=12)\nplt.ylabel('Relative L2 Error', fontsize=12)\nplt.title('Error vs Computational Cost', fontsize=14)\nplt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.savefig('error_vs_time.png', dpi=300, bbox_inches='tight')\nplt.savefig('error_vs_time.pdf', dpi=300, bbox_inches='tight')\nplt.show()\n\n# 3. Compute and plot efficiency metrics\ndf_crbe['efficiency'] = df_crbe['rel_l2_error'] * df_crbe['train_time']\ndf_pinn['efficiency'] = df_pinn['rel_l2_error'] * df_pinn['train_time']\n\nplt.figure(figsize=(10, 6))\nplt.loglog(df_crbe['mesh_size'], df_crbe['efficiency'], 'o-', linewidth=2, markersize=8, label='CR-BE')\nplt.loglog(df_pinn['mesh_size'], df_pinn['efficiency'], 's-', linewidth=2, markersize=8, label='PINN')\n\nplt.xlabel('Mesh Size', fontsize=12)\nplt.ylabel('Efficiency Metric (Error × Running Time)', fontsize=12)\nplt.title('Computational Efficiency Comparison', fontsize=14)\nplt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\nplt.legend(fontsize=10)\nplt.tight_layout()\nplt.savefig('efficiency_comparison.png', dpi=300, bbox_inches='tight')\nplt.savefig('efficiency_comparison.pdf', dpi=300, bbox_inches='tight')\nplt.show()\n\n# 4. Memory usage estimation (approximate based on number of parameters/DOFs)\n# Note: This is an approximation; actual memory would need profiling\nif 'n_parameters' in df_pinn:\n    df_crbe['memory_estimate'] = np.array(n_dofs) * 8 / 1024  # KB (8 bytes per double)\n    df_pinn['memory_estimate'] = df_pinn['n_parameters'] * 4 / 1024  # KB (4 bytes per float32 parameter)\n    \n    plt.figure(figsize=(10, 6))\n    plt.loglog(df_crbe['memory_estimate'], df_crbe['rel_l2_error'], 'o-', linewidth=2, markersize=8, label='CR-BE')\n    plt.loglog(df_pinn['memory_estimate'], df_pinn['rel_l2_error'], 's-', linewidth=2, markersize=8, label='PINN')\n    \n    plt.xlabel('Estimated Memory Usage (KB)', fontsize=12)\n    plt.ylabel('Relative L2 Error', fontsize=12)\n    plt.title('Error vs Memory Usage', fontsize=14)\n    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n    plt.legend(fontsize=10)\n    plt.tight_layout()\n    plt.savefig('error_vs_memory.png', dpi=300, bbox_inches='tight')\n    plt.savefig('error_vs_memory.pdf', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# 5. Visualization of solution comparison at highest resolution\n# Assuming we can extract solutions at final time point on a grid\n\ndef plot_comparison_at_final_time(mesh_size_idx=-1):\n    \"\"\"Plot solution comparison at the final time using the specified mesh size index\"\"\"\n    # This would need to be adapted based on how you store solutions\n    # Here's a placeholder structure:\n    \n    # Extract or recompute solutions on a common grid\n    n_viz = 100\n    x_viz = np.linspace(-domain_size, domain_size, n_viz)\n    y_viz = np.linspace(-domain_size, domain_size, n_viz)\n    X_viz, Y_viz = np.meshgrid(x_viz, y_viz)\n    \n    # Get the mesh size for the plot\n    mesh_size = mesh_sizes[mesh_size_idx]\n    \n    # Placeholder: You'll need to implement the solution extraction or recomputation\n    # For example:\n    # cr_solution = extract_solution(solver, T, X_viz, Y_viz)\n    # pinn_solution = model.predict(np.column_stack((np.full(X_viz.size, T), X_viz.flatten(), Y_viz.flatten()))).reshape(X_viz.shape)\n    # exact_solution = np.array([exact_sol_fn(T, x, y) for x, y in zip(X_viz.flatten(), Y_viz.flatten())]).reshape(X_viz.shape)\n    \n    # For demonstration, let's create some placeholder data\n    # Replace these with actual solution evaluations\n    t_eval = T\n    exact_solution = np.zeros((n_viz, n_viz))\n    cr_solution = np.zeros((n_viz, n_viz))\n    pinn_solution = np.zeros((n_viz, n_viz))\n    \n    for i in range(n_viz):\n        for j in range(n_viz):\n            x, y = X_viz[i, j], Y_viz[i, j]\n            exact_solution[i, j] = exact_sol_fn(t_eval, x, y)\n            # You would need to implement these:\n            # cr_solution[i, j] = cr_solution_at_point(t_eval, x, y)\n            # pinn_solution[i, j] = pinn_solution_at_point(t_eval, x, y)\n    \n    # Create grid of error plots\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10), sharex=True, sharey=True)\n    \n    # Plot solutions\n    levels = np.linspace(np.min(exact_solution), np.max(exact_solution), 50)\n    im1 = axes[0, 0].contourf(X_viz, Y_viz, exact_solution, levels=levels, cmap='viridis')\n    im2 = axes[0, 1].contourf(X_viz, Y_viz, cr_solution, levels=levels, cmap='viridis')\n    im3 = axes[0, 2].contourf(X_viz, Y_viz, pinn_solution, levels=levels, cmap='viridis')\n    \n    axes[0, 0].set_title(f'Exact Solution at t={T}', fontsize=12)\n    axes[0, 1].set_title(f'CR-BE Solution (mesh={mesh_size})', fontsize=12)\n    axes[0, 2].set_title(f'PINN Solution (n={mesh_size}²)', fontsize=12)\n    \n    # Plot errors\n    cr_error = np.abs(cr_solution - exact_solution)\n    pinn_error = np.abs(pinn_solution - exact_solution)\n    error_levels = np.linspace(0, max(np.max(cr_error), np.max(pinn_error)), 50)\n    \n    im4 = axes[1, 0].contourf(X_viz, Y_viz, np.zeros_like(X_viz), levels=error_levels, cmap='magma')  # placeholder\n    im5 = axes[1, 1].contourf(X_viz, Y_viz, cr_error, levels=error_levels, cmap='magma')\n    im6 = axes[1, 2].contourf(X_viz, Y_viz, pinn_error, levels=error_levels, cmap='magma')\n    \n    axes[1, 0].set_title('Reference (Zero Error)', fontsize=12)\n    axes[1, 1].set_title(f'CR-BE Error (L∞={np.max(cr_error):.2e})', fontsize=12)\n    axes[1, 2].set_title(f'PINN Error (L∞={np.max(pinn_error):.2e})', fontsize=12)\n    \n    # Add colorbars\n    cbar1 = fig.colorbar(im1, ax=axes[0, :], orientation='horizontal', pad=0.1, aspect=30)\n    cbar1.set_label('Solution Value')\n    \n    cbar2 = fig.colorbar(im5, ax=axes[1, :], orientation='horizontal', pad=0.1, aspect=30)\n    cbar2.set_label('Absolute Error')\n    \n    # Add labels\n    for i in range(2):\n        for j in range(3):\n            axes[i, j].set_xlabel('x', fontsize=10)\n            axes[i, j].set_ylabel('y', fontsize=10)\n    \n    plt.tight_layout()\n    plt.savefig('solution_comparison.png', dpi=300, bbox_inches='tight')\n    plt.savefig('solution_comparison.pdf', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Uncomment this when you have the solutions available\n# plot_comparison_at_final_time()\n\n# 6. Parameter sensitivity analysis\n# This would run multiple convergence tests with different parameters\n# Here's a simplified example for diffusion coefficient sensitivity\n\ndef run_parameter_sensitivity_analysis():\n    \"\"\"Example of how to run parameter sensitivity analysis\"\"\"\n    # Parameters to test\n    diffusion_coefficients = [0.01, 0.1, 1.0]\n    mesh_selection = [8, 32, 128]  # Selected mesh sizes to test\n    \n    # Results storage\n    sensitivity_results = []\n    \n    for D_value in diffusion_coefficients:\n        print(f\"Testing diffusion coefficient D = {D_value}\")\n        \n        # Update model parameters\n        model_params = crbe_fem.Models(vx=vx, vy=vy, D=D_value, sigma=sigma)\n        \n        for mesh_size in mesh_selection:\n            # Run CR-BE solver with this configuration\n            mesh_file = crbe_fem.create_mesh(mesh_size, domain_size=domain_size)\n            mesh = meshio.read(mesh_file)\n            mesh_data = crbe_fem.MeshData(mesh, domain_params, nt=n_steps)\n            \n            solver = crbe_fem.BESCRFEM(domain_params, model_params, mesh_data, use_quadrature=True)\n            solver.solve()\n            \n            errors = solver.compute_errors()\n            \n            # Store results\n            sensitivity_results.append({\n                \"method\": \"CR-BE\",\n                \"diffusion\": D_value,\n                \"mesh_size\": mesh_size,\n                \"rel_l2_error\": errors['final_l2_error'],\n                \"max_error\": errors['final_linf_error']\n            })\n            \n            # Similarly for PINN (would need to implement)\n            # ...\n    \n    # Convert to DataFrame and analyze\n    df_sensitivity = pd.DataFrame(sensitivity_results)\n    \n    # Plot sensitivity results\n    plt.figure(figsize=(12, 8))\n    sns.lineplot(data=df_sensitivity, x='mesh_size', y='rel_l2_error', \n                 hue='diffusion', style='method', markers=True, dashes=False)\n    \n    plt.xscale('log')\n    plt.yscale('log')\n    plt.xlabel('Mesh Size', fontsize=12)\n    plt.ylabel('Relative L2 Error', fontsize=12)\n    plt.title('Sensitivity to Diffusion Coefficient', fontsize=14)\n    plt.grid(True, which=\"both\", ls=\"--\", alpha=0.7)\n    plt.legend(title='Diffusion Coef. (D)', fontsize=10)\n    plt.tight_layout()\n    plt.savefig('parameter_sensitivity.png', dpi=300, bbox_inches='tight')\n    plt.savefig('parameter_sensitivity.pdf', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Uncomment to run sensitivity analysis (this will take time!)\n# run_parameter_sensitivity_analysis()\n\n# 7. Visualization of all result metrics in a comprehensive dashboard\ndef create_results_dashboard():\n    \"\"\"Create a comprehensive dashboard with all metrics\"\"\"\n    # Combine all results\n    df_combined = pd.concat([\n        df_crbe.assign(method='CR-BE'),\n        df_pinn.assign(method='PINN')\n    ])\n    \n    # Create figure with multiple subplots\n    fig = plt.figure(figsize=(18, 12))\n    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n    \n    # 1. L2 Error convergence\n    ax1 = fig.add_subplot(gs[0, 0])\n    for method, color, marker in zip(['CR-BE', 'PINN'], ['blue', 'red'], ['o', 's']):\n        df_method = df_combined[df_combined['method'] == method]\n        ax1.loglog(df_method['mesh_size'], df_method['rel_l2_error'], \n                  marker=marker, linestyle='-', color=color, label=method)\n    \n    # Add reference lines\n    x_ref = np.array([min(mesh_sizes), max(mesh_sizes)])\n    ax1.loglog(x_ref, 1/x_ref, 'k--', alpha=0.5, label='O(h)')\n    ax1.loglog(x_ref, 1/x_ref**2, 'k-.', alpha=0.5, label='O(h²)')\n    \n    ax1.set_xlabel('Mesh Size')\n    ax1.set_ylabel('L2 Error')\n    ax1.set_title('Error Convergence (L2)')\n    ax1.grid(True, which='both', ls='--', alpha=0.6)\n    ax1.legend()\n    \n    # 2. L∞ Error convergence\n    ax2 = fig.add_subplot(gs[0, 1])\n    for method, color, marker in zip(['CR-BE', 'PINN'], ['blue', 'red'], ['o', 's']):\n        df_method = df_combined[df_combined['method'] == method]\n        ax2.loglog(df_method['mesh_size'], df_method['max_error'], \n                  marker=marker, linestyle='-', color=color, label=method)\n    \n    # Add reference lines\n    ax2.loglog(x_ref, 1/x_ref, 'k--', alpha=0.5, label='O(h)')\n    ax2.loglog(x_ref, 1/x_ref**2, 'k-.', alpha=0.5, label='O(h²)')\n    \n    ax2.set_xlabel('Mesh Size')\n    ax2.set_ylabel('L∞ Error')\n    ax2.set_title('Error Convergence (L∞)')\n    ax2.grid(True, which='both', ls='--', alpha=0.6)\n    ax2.legend()\n    \n    # 3. Computational time\n    ax3 = fig.add_subplot(gs[0, 2])\n    for method, color, marker in zip(['CR-BE', 'PINN'], ['blue', 'red'], ['o', 's']):\n        df_method = df_combined[df_combined['method'] == method]\n        ax3.loglog(df_method['mesh_size'], df_method['train_time'], \n                  marker=marker, linestyle='-', color=color, label=method)\n    \n    ax3.set_xlabel('Mesh Size')\n    ax3.set_ylabel('Running Time (s)')\n    ax3.set_title('Computational Cost')\n    ax3.grid(True, which='both', ls='--', alpha=0.6)\n    ax3.legend()\n    \n    # 4. Error vs Time\n    ax4 = fig.add_subplot(gs[1, 0])\n    for method, color, marker in zip(['CR-BE', 'PINN'], ['blue', 'red'], ['o', 's']):\n        df_method = df_combined[df_combined['method'] == method]\n        ax4.loglog(df_method['train_time'], df_method['rel_l2_error'], \n                  marker=marker, linestyle='-', color=color, label=method)\n    \n    ax4.set_xlabel('Running Time (s)')\n    ax4.set_ylabel('L2 Error')\n    ax4.set_title('Efficiency (Error vs Time)')\n    ax4.grid(True, which='both', ls='--', alpha=0.6)\n    ax4.legend()\n    \n    # 5. Efficiency metric\n    ax5 = fig.add_subplot(gs[1, 1])\n    for method, color, marker in zip(['CR-BE', 'PINN'], ['blue', 'red'], ['o', 's']):\n        df_method = df_combined[df_combined['method'] == method]\n        ax5.loglog(df_method['mesh_size'], df_method['rel_l2_error'] * df_method['train_time'], \n                  marker=marker, linestyle='-', color=color, label=method)\n    \n    ax5.set_xlabel('Mesh Size')\n    ax5.set_ylabel('L2 Error × Running Time')\n    ax5.set_title('Efficiency Metric')\n    ax5.grid(True, which='both', ls='--', alpha=0.6)\n    ax5.legend()\n    \n    # 6. Memory usage (if available)\n    if 'memory_estimate' in df_combined.columns:\n        ax6 = fig.add_subplot(gs[1, 2])\n        for method, color, marker in zip(['CR-BE', 'PINN'], ['blue', 'red'], ['o', 's']):\n            df_method = df_combined[df_combined['method'] == method]\n            ax6.loglog(df_method['mesh_size'], df_method['memory_estimate'], \n                    marker=marker, linestyle='-', color=color, label=method)\n        \n        ax6.set_xlabel('Mesh Size')\n        ax6.set_ylabel('Memory (KB)')\n        ax6.set_title('Memory Usage')\n        ax6.grid(True, which='both', ls='--', alpha=0.6)\n        ax6.legend()\n    \n    # 7. L2 Error normalized by DOFs\n    ax7 = fig.add_subplot(gs[2, 0])\n    for method, color, marker in zip(['CR-BE', 'PINN'], ['blue', 'red'], ['o', 's']):\n        df_method = df_combined[df_combined['method'] == method]\n        dofs = df_method['number_of_collocation_points']\n        ax7.loglog(dofs, df_method['rel_l2_error'], \n                  marker=marker, linestyle='-', color=color, label=method)\n    \n    ax7.set_xlabel('Degrees of Freedom / Collocation Points')\n    ax7.set_ylabel('L2 Error')\n    ax7.set_title('Error vs Problem Size')\n    ax7.grid(True, which='both', ls='--', alpha=0.6)\n    ax7.legend()\n    \n    # 8. Bar chart of convergence rates\n    ax8 = fig.add_subplot(gs[2, 1])\n    methods = ['CR-BE', 'PINN']\n    l2_rates = [cr_l2_rate, pinn_l2_rate]\n    linf_rates = [cr_linf_rate, pinn_linf_rate]\n    \n    x = np.arange(len(methods))\n    width = 0.35\n    \n    ax8.bar(x - width/2, l2_rates, width, label='L2 Rate')\n    ax8.bar(x + width/2, linf_rates, width, label='L∞ Rate')\n    \n    ax8.set_ylabel('Convergence Rate')\n    ax8.set_title('Empirical Convergence Rates')\n    ax8.set_xticks(x)\n    ax8.set_xticklabels(methods)\n    ax8.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='First Order')\n    ax8.axhline(y=2.0, color='k', linestyle='-.', alpha=0.5, label='Second Order')\n    ax8.legend()\n    \n    # 9. Summary statistics table\n    ax9 = fig.add_subplot(gs[2, 2])\n    ax9.axis('off')\n    \n    # Create summary table\n    table_data = [\n        ['Method', 'CR-BE', 'PINN'],\n        ['L2 Rate', f\"{cr_l2_rate:.3f}\", f\"{pinn_l2_rate:.3f}\"],\n        ['L∞ Rate', f\"{cr_linf_rate:.3f}\", f\"{pinn_linf_rate:.3f}\"],\n        ['Min L2 Error', f\"{df_crbe['rel_l2_error'].min():.2e}\", f\"{df_pinn['rel_l2_error'].min():.2e}\"],\n        ['Min L∞ Error', f\"{df_crbe['max_error'].min():.2e}\", f\"{df_pinn['max_error'].min():.2e}\"],\n        ['Max Time (s)', f\"{df_crbe['train_time'].max():.2f}\", f\"{df_pinn['train_time'].max():.2f}\"]\n    ]\n    \n    table = ax9.table(cellText=table_data, loc='center', cellLoc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1, 1.5)\n    ax9.set_title('Summary Statistics')\n    \n    plt.tight_layout()\n    plt.savefig('results_dashboard.png', dpi=300, bbox_inches='tight')\n    plt.savefig('results_dashboard.pdf', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create the comprehensive dashboard\ncreate_results_dashboard()\n\n# 8. Export results to CSV for further analysis\ndf_crbe.to_csv('crbe_results.csv', index=False)\ndf_pinn.to_csv('pinn_results.csv', index=False)\ndf_combined = pd.concat([\n    df_crbe.assign(method='CR-BE'),\n    df_pinn.assign(method='PINN')\n])\ndf_combined.to_csv('combined_results.csv', index=False)\n\nprint(\"Advanced convergence analysis completed!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Convergence Tables","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"memory_data = pd.DataFrame(\n    {\n        \"cr_memory_mb\": list(df_crbe[\"cpu_memory_usage_MB\"].values),\n        \"pinn_memory_mb\": list(df_pinn[\"gpu_memory_usage_MB\"].values)\n    }\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_latex_tables(df_crbe, df_pinn, memory_data=None, sensitivity_data=None):\n    \"\"\"\n    Generate LaTeX tables from DataFrame results.\n\n    Args:\n        df_crbe: DataFrame with CR-BE results\n        df_pinn: DataFrame with PINN results\n        memory_data: Optional DataFrame with memory usage\n        sensitivity_data: Optional DataFrame with sensitivity analysis\n\n    Returns:\n        dict: Dictionary of LaTeX table strings\n    \"\"\"\n    import numpy as np\n    from scipy.stats import linregress\n\n    def format_sci(x):\n        if abs(x) < 0.01:\n            s = f\"{x:.2e}\"\n            base, exp = s.split('e')\n            exp = int(exp)\n            return f\"${base}\\\\times 10^{{{exp}}}$\"\n        else:\n            return f\"${x:.4f}$\"\n\n    tables = {}\n\n    mesh_sizes = df_crbe['mesh_size'].values\n\n    log_h_crbe = np.log(1 / df_crbe['mesh_size'].values)\n    log_err_l2_crbe = np.log(df_crbe['rel_l2_error'].values)\n    log_err_linf_crbe = np.log(df_crbe['max_error'].values)\n\n    log_h_pinn = np.log(1 / df_pinn['mesh_size'].values)\n    log_err_l2_pinn = np.log(df_pinn['rel_l2_error'].values)\n    log_err_linf_pinn = np.log(df_pinn['max_error'].values)\n\n    crbe_l2_rate, _, crbe_l2_r2, _, _ = linregress(log_h_crbe, log_err_l2_crbe)\n    crbe_linf_rate, _, crbe_linf_r2, _, _ = linregress(log_h_crbe, log_err_linf_crbe)\n    pinn_l2_rate, _, pinn_l2_r2, _, _ = linregress(log_h_pinn, log_err_l2_pinn)\n    pinn_linf_rate, _, pinn_linf_r2, _, _ = linregress(log_h_pinn, log_err_linf_pinn)\n\n    # Table 1: Convergence comparison\n    table1 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table1 += \"\\\\caption{Convergence comparison of CR-BE and PINN}\\n\"\n    table1 += \"\\\\label{tab:convergence_comparison}\\n\"\n    table1 += \"\\\\begin{tabular}{ccccccc}\\n\\\\toprule\\n\"\n    table1 += \"Mesh Size & CR-BE $L^2$ & PINN $L^2$ & CR-BE $L^\\\\infty$ & PINN $L^\\\\infty$ & CR-BE Time & PINN Time \\\\\\\\\\n\\\\midrule\\n\"\n    for i, mesh in enumerate(mesh_sizes):\n        table1 += f\"{mesh} & {format_sci(df_crbe['rel_l2_error'].iloc[i])} & {format_sci(df_pinn['rel_l2_error'].iloc[i])} & \"\n        table1 += f\"{format_sci(df_crbe['max_error'].iloc[i])} & {format_sci(df_pinn['max_error'].iloc[i])} & \"\n        table1 += f\"${df_crbe['train_time'].iloc[i]:.2f}$ & ${df_pinn['train_time'].iloc[i]:.2f}$ \\\\\\\\\\n\"\n    table1 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    tables['convergence'] = table1\n\n    # Table 2: Convergence rates\n    table2 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table2 += \"\\\\caption{Empirical convergence rates}\\n\"\n    table2 += \"\\\\label{tab:convergence_rates}\\n\"\n    table2 += \"\\\\begin{tabular}{ccccc}\\n\\\\toprule\\n\"\n    table2 += \"Method & $L^2$ Rate & $L^\\\\infty$ Rate & $R^2(L^2)$ & $R^2(L^\\\\infty)$ \\\\\\\\\\n\\\\midrule\\n\"\n    table2 += f\"CR-BE & ${crbe_l2_rate:.3f}$ & ${crbe_linf_rate:.3f}$ & ${crbe_l2_r2:.3f}$ & ${crbe_linf_r2:.3f}$ \\\\\\\\\\n\"\n    table2 += f\"PINN & ${pinn_l2_rate:.3f}$ & ${pinn_linf_rate:.3f}$ & ${pinn_l2_r2:.3f}$ & ${pinn_linf_r2:.3f}$ \\\\\\\\\\n\"\n    table2 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    tables['rates'] = table2\n\n    # Table 3: Computational resources\n    table3 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table3 += \"\\\\caption{Computational resource requirements}\\n\"\n    table3 += \"\\\\label{tab:computational_resources}\\n\"\n    table3 += \"\\\\begin{tabular}{ccccc}\\n\\\\toprule\\n\"\n    table3 += \"Mesh Size & CR-BE Memory & PINN Memory & CR-BE DOFs & PINN Params \\\\\\\\\\n\\\\midrule\\n\"\n    for i, mesh in enumerate(mesh_sizes):\n        mem_crbe = f\"${memory_data['cr_memory_mb'].iloc[i]:.1f}$\" if memory_data is not None else \"$-$\"\n        mem_pinn = f\"${memory_data['pinn_memory_mb'].iloc[i]:.1f}$\" if memory_data is not None else \"$-$\"\n        dofs_crbe = f\"${df_crbe['number_of_collocation_points'].iloc[i]}$\"\n        params_pinn = f\"${df_pinn['n_parameters'].iloc[i]}$\" if 'n_parameters' in df_pinn.columns else \"$-$\"\n        table3 += f\"{mesh} & {mem_crbe} & {mem_pinn} & {dofs_crbe} & {params_pinn} \\\\\\\\\\n\"\n    table3 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    tables['resources'] = table3\n\n    # Table 4: Efficiency (Error × Time)\n    table4 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table4 += \"\\\\caption{Efficiency: Error × Time}\\n\"\n    table4 += \"\\\\label{tab:efficiency}\\n\"\n    table4 += \"\\\\begin{tabular}{ccc}\\n\\\\toprule\\n\"\n    table4 += \"Mesh Size & CR-BE Efficiency & PINN Efficiency \\\\\\\\\\n\\\\midrule\\n\"\n    for i, mesh in enumerate(mesh_sizes):\n        eff_crbe = df_crbe['rel_l2_error'].iloc[i] * df_crbe['train_time'].iloc[i]\n        eff_pinn = df_pinn['rel_l2_error'].iloc[i] * df_pinn['train_time'].iloc[i]\n        table4 += f\"{mesh} & ${eff_crbe:.2e}$ & ${eff_pinn:.2e}$ \\\\\\\\\\n\"\n    table4 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    tables['efficiency'] = table4\n\n    # Table 5: Summary\n    table5 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table5 += \"\\\\caption{Summary of method performance}\\n\"\n    table5 += \"\\\\label{tab:summary}\\n\"\n    table5 += \"\\\\begin{tabular}{lcc}\\n\\\\toprule\\n\"\n    table5 += \"Metric & CR-BE & PINN \\\\\\\\\\n\\\\midrule\\n\"\n    table5 += f\"Minimum $L^2$ Error & {format_sci(df_crbe['rel_l2_error'].min())} & {format_sci(df_pinn['rel_l2_error'].min())} \\\\\\\\\\n\"\n    table5 += f\"Minimum $L^\\\\infty$ Error & {format_sci(df_crbe['max_error'].min())} & {format_sci(df_pinn['max_error'].min())} \\\\\\\\\\n\"\n    table5 += f\"Maximum Training Time (s) & ${df_crbe['train_time'].max():.2f}$ & ${df_pinn['train_time'].max():.2f}$ \\\\\\\\\\n\"\n    table5 += f\"$L^2$ Convergence Rate & ${crbe_l2_rate:.3f}$ & ${pinn_l2_rate:.3f}$ \\\\\\\\\\n\"\n    table5 += f\"$L^\\\\infty$ Convergence Rate & ${crbe_linf_rate:.3f}$ & ${pinn_linf_rate:.3f}$ \\\\\\\\\\n\"\n    table5 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    tables['summary'] = table5\n\n    # Table 6: Memory Usage (separately)\n    if memory_data is not None:\n        table6 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n        table6 += \"\\\\caption{Memory usage during training}\\n\"\n        table6 += \"\\\\label{tab:memory_usage}\\n\"\n        table6 += \"\\\\begin{tabular}{ccc}\\n\\\\toprule\\n\"\n        table6 += \"Mesh Size & CR-BE (MB) & PINN (MB) \\\\\\\\\\n\\\\midrule\\n\"\n        for i, mesh in enumerate(mesh_sizes):\n            mem_crbe = f\"${memory_data['cr_memory_mb'].iloc[i]:.1f}$\"\n            mem_pinn = f\"${memory_data['pinn_memory_mb'].iloc[i]:.1f}$\"\n            table6 += f\"{mesh} & {mem_crbe} & {mem_pinn} \\\\\\\\\\n\"\n        table6 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n        tables['memory'] = table6\n\n    # Table 7: Sensitivity to diffusion coefficient\n    if sensitivity_data is not None:\n        table7 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n        table7 += \"\\\\caption{Sensitivity to diffusion coefficient variations}\\n\"\n        table7 += \"\\\\label{tab:sensitivity_diffusion}\\n\"\n        table7 += \"\\\\begin{tabular}{ccc}\\n\\\\toprule\\n\"\n        table7 += \"Diffusion Coefficient & CR-BE $L^2$ Error & PINN $L^2$ Error \\\\\\\\\\n\\\\midrule\\n\"\n        for idx, row in sensitivity_data.iterrows():\n            table7 += f\"${row['diffusion_coef']:.2e}$ & {format_sci(row['cr_l2_error'])} & {format_sci(row['pinn_l2_error'])} \\\\\\\\\\n\"\n        table7 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n        tables['sensitivity'] = table7\n\n    return tables\n\n# Example usage\n# Generate tables from your results\ntables = generate_latex_tables(df_crbe, df_pinn, memory_data, df_sensitivity_data)\n    \n# Write tables to file\nwith open('convergence_tables.tex', 'w') as f:\n    for name, table in tables.items():\n        f.write(f\"% {name}\\n\")\n        f.write(table)\n        f.write(\"\\n\\n\")\n            \nprint(\"LaTeX tables generated and saved to convergence_tables.tex\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_latex_tables(df_crbe, df_pinn, memory_data=None, sensitivity_data=None):\n    \"\"\"\n    Generate LaTeX tables from DataFrame results\n    \n    Args:\n        df_crbe: DataFrame with CR-BE results\n        df_pinn: DataFrame with PINN results\n        memory_data: Optional DataFrame with memory measurements\n        sensitivity_data: Optional DataFrame with sensitivity analysis\n        \n    Returns:\n        dict: Dictionary of LaTeX table strings\n    \"\"\"\n    import numpy as np\n    from scipy.stats import linregress\n    \n    # Helper function to format scientific notation nicely for LaTeX\n    def format_sci(x):\n        if abs(x) < 0.01:\n            return f\"${x:.2e}\".replace(\"e-0\", \"\\\\times 10^{-\").replace(\"e-\", \"\\\\times 10^{-\") + \"}$\"\n        else:\n            return f\"${x:.4f}$\"\n    \n    # Calculate convergence rates\n    mesh_sizes = df_crbe['mesh_size'].values\n    \n    log_h_crbe = np.log(1/df_crbe['mesh_size'].values)\n    log_err_l2_crbe = np.log(df_crbe['rel_l2_error'].values)\n    log_err_linf_crbe = np.log(df_crbe['max_error'].values)\n    \n    log_h_pinn = np.log(1/df_pinn['mesh_size'].values)\n    log_err_l2_pinn = np.log(df_pinn['rel_l2_error'].values)\n    log_err_linf_pinn = np.log(df_pinn['max_error'].values)\n    \n    # Linear regression to find slopes (convergence rates)\n    crbe_l2_rate, _, crbe_l2_r2, _, _ = linregress(log_h_crbe, log_err_l2_crbe)\n    crbe_linf_rate, _, crbe_linf_r2, _, _ = linregress(log_h_crbe, log_err_linf_crbe)\n    pinn_l2_rate, _, pinn_l2_r2, _, _ = linregress(log_h_pinn, log_err_l2_pinn)\n    pinn_linf_rate, _, pinn_linf_r2, _, _ = linregress(log_h_pinn, log_err_linf_pinn)\n    \n    # Table 1: Convergence comparison\n    table1 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table1 += \"\\\\caption{Convergence comparison of CR-BE and PINN methods}\\n\"\n    table1 += \"\\\\label{tab:convergence_comparison}\\n\"\n    table1 += \"\\\\begin{tabular}{ccccccc}\\n\\\\toprule\\n\"\n    table1 += \"\\\\multirow{2}{*}{Mesh Size} & \\\\multicolumn{2}{c}{Relative $L^2$ Error} & \"\n    table1 += \"\\\\multicolumn{2}{c}{Maximum Error ($L^\\\\infty$)} & \\\\multicolumn{2}{c}{Training Time (s)} \\\\\\\\\\n\"\n    table1 += \"\\\\cmidrule(lr){2-3} \\\\cmidrule(lr){4-5} \\\\cmidrule(lr){6-7}\\n\"\n    table1 += \"& CR-BE & PINN & CR-BE & PINN & CR-BE & PINN \\\\\\\\\\n\\\\midrule\\n\"\n    \n    for i, mesh in enumerate(mesh_sizes):\n        cr_l2 = format_sci(df_crbe['rel_l2_error'].iloc[i])\n        pinn_l2 = format_sci(df_pinn['rel_l2_error'].iloc[i])\n        cr_linf = format_sci(df_crbe['max_error'].iloc[i])\n        pinn_linf = format_sci(df_pinn['max_error'].iloc[i])\n        cr_time = f\"${df_crbe['train_time'].iloc[i]:.2f}$\"\n        pinn_time = f\"${df_pinn['train_time'].iloc[i]:.2f}$\"\n        \n        table1 += f\"{mesh} & {cr_l2} & {pinn_l2} & {cr_linf} & {pinn_linf} & {cr_time} & {pinn_time} \\\\\\\\\\n\"\n    \n    table1 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    \n    # Table 2: Convergence rates\n    table2 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table2 += \"\\\\caption{Empirical convergence rates for CR-BE and PINN methods}\\n\"\n    table2 += \"\\\\label{tab:convergence_rates}\\n\"\n    table2 += \"\\\\begin{tabular}{ccccc}\\n\\\\toprule\\n\"\n    table2 += \"\\\\multirow{2}{*}{Method} & \\\\multicolumn{2}{c}{Convergence Rate} & \"\n    table2 += \"\\\\multicolumn{2}{c}{Goodness of Fit ($R^2$)} \\\\\\\\\\n\"\n    table2 += \"\\\\cmidrule(lr){2-3} \\\\cmidrule(lr){4-5}\\n\"\n    table2 += \"& $L^2$ Error & $L^\\\\infty$ Error & $L^2$ Error & $L^\\\\infty$ Error \\\\\\\\\\n\\\\midrule\\n\"\n    \n    table2 += f\"CR-BE & ${crbe_l2_rate:.3f}$ & ${crbe_linf_rate:.3f}$ & \"\n    table2 += f\"${crbe_l2_r2:.3f}$ & ${crbe_linf_r2:.3f}$ \\\\\\\\\\n\"\n    \n    table2 += f\"PINN & ${pinn_l2_rate:.3f}$ & ${pinn_linf_rate:.3f}$ & \"\n    table2 += f\"${pinn_l2_r2:.3f}$ & ${pinn_linf_r2:.3f}$ \\\\\\\\\\n\"\n    \n    table2 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    \n    # Table 3: Computational resources (if memory data available)\n    table3 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table3 += \"\\\\caption{Computational resource requirements}\\n\"\n    table3 += \"\\\\label{tab:computational_resources}\\n\"\n    table3 += \"\\\\begin{tabular}{ccccc}\\n\\\\toprule\\n\"\n    table3 += \"\\\\multirow{2}{*}{Mesh Size} & \\\\multicolumn{2}{c}{Memory Usage (MB)} & \"\n    table3 += \"\\\\multicolumn{2}{c}{DOFs / Parameters} \\\\\\\\\\n\"\n    table3 += \"\\\\cmidrule(lr){2-3} \\\\cmidrule(lr){4-5}\\n\"\n    table3 += \"& CR-BE & PINN & CR-BE & PINN \\\\\\\\\\n\\\\midrule\\n\"\n    \n    # If we have memory data\n    if memory_data is not None:\n        for i, mesh in enumerate(mesh_sizes):\n            mem_crbe = f\"${memory_data['cr_memory_mb'].iloc[i]:.2f}$\"\n            mem_pinn = f\"${memory_data['pinn_memory_mb'].iloc[i]:.2f}$\"\n            dofs_crbe = f\"${df_crbe['number_of_collocation_points'].iloc[i]}$\"\n            \n            # Check if we have parameter counts for PINN\n            if 'n_parameters' in df_pinn.columns:\n                params_pinn = f\"${df_pinn['n_parameters'].iloc[i]}$\"\n            else:\n                params_pinn = \"$-$\"\n                \n            table3 += f\"{mesh} & {mem_crbe} & {mem_pinn} & {dofs_crbe} & {params_pinn} \\\\\\\\\\n\"\n    else:\n        # If no memory data, use placeholder or skip\n        for i, mesh in enumerate(mesh_sizes):\n            dofs_crbe = f\"${df_crbe['number_of_collocation_points'].iloc[i]}$\"\n            \n            # Check if we have parameter counts for PINN\n            if 'n_parameters' in df_pinn.columns:\n                params_pinn = f\"${df_pinn['n_parameters'].iloc[i]}$\"\n            else:\n                params_pinn = \"$-$\"\n                \n            table3 += f\"{mesh} & $-$ & $-$ & {dofs_crbe} & {params_pinn} \\\\\\\\\\n\"\n    \n    table3 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    \n    # Table 4: Efficiency comparison\n    table4 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table4 += \"\\\\caption{Efficiency comparison ($L^2$ error $\\\\times$ training time)}\\n\"\n    table4 += \"\\\\label{tab:efficiency_comparison}\\n\"\n    table4 += \"\\\\begin{tabular}{ccc}\\n\\\\toprule\\n\"\n    table4 += \"Mesh Size & CR-BE Efficiency & PINN Efficiency \\\\\\\\\\n\\\\midrule\\n\"\n    \n    for i, mesh in enumerate(mesh_sizes):\n        eff_crbe = df_crbe['rel_l2_error'].iloc[i] * df_crbe['train_time'].iloc[i]\n        eff_pinn = df_pinn['rel_l2_error'].iloc[i] * df_pinn['train_time'].iloc[i]\n        \n        table4 += f\"{mesh} & ${eff_crbe:.2e}$ & ${eff_pinn:.2e}$ \\\\\\\\\\n\"\n    \n    table4 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    \n    # Table 5: Summary statistics\n    table5 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table5 += \"\\\\caption{Summary of method performance}\\n\"\n    table5 += \"\\\\label{tab:summary_statistics}\\n\"\n    table5 += \"\\\\begin{tabular}{lcc}\\n\\\\toprule\\n\"\n    table5 += \"Metric & CR-BE & PINN \\\\\\\\\\n\\\\midrule\\n\"\n    \n    min_l2_crbe = format_sci(df_crbe['rel_l2_error'].min())\n    min_l2_pinn = format_sci(df_pinn['rel_l2_error'].min())\n    min_linf_crbe = format_sci(df_crbe['max_error'].min())\n    min_linf_pinn = format_sci(df_pinn['max_error'].min())\n    max_time_crbe = f\"${df_crbe['train_time'].max():.2f}$\"\n    max_time_pinn = f\"${df_pinn['train_time'].max():.2f}$\"\n    \n    table5 += f\"Minimum $L^2$ Error & {min_l2_crbe} & {min_l2_pinn} \\\\\\\\\\n\"\n    table5 += f\"Minimum $L^\\\\infty$ Error & {min_linf_crbe} & {min_linf_pinn} \\\\\\\\\\n\"\n    table5 += f\"Maximum Training Time (s) & {max_time_crbe} & {max_time_pinn} \\\\\\\\\\n\"\n    table5 += f\"$L^2$ Convergence Rate & ${crbe_l2_rate:.3f}$ & ${pinn_l2_rate:.3f}$ \\\\\\\\\\n\"\n    table5 += f\"$L^\\\\infty$ Convergence Rate & ${crbe_linf_rate:.3f}$ & ${pinn_linf_rate:.3f}$ \\\\\\\\\\n\"\n    \n    # Determine scaling based on convergence rate\n    scaling_crbe = f\"$O(n^{{{abs(crbe_l2_rate):.1f}}})$\"\n    scaling_pinn = f\"$O(n^{{{abs(pinn_l2_rate):.1f}}})$\"\n    table5 += f\"Error Scaling & {scaling_crbe} & {scaling_pinn} \\\\\\\\\\n\"\n    \n    table5 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    \n    # Table 6: Method characteristics\n    table6 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n    table6 += \"\\\\caption{Quantitative evidence for method characteristics}\\n\"\n    table6 += \"\\\\label{tab:method_characteristics}\\n\"\n    table6 += \"\\\\begin{tabular}{lcc}\\n\\\\toprule\\n\"\n    table6 += \"Characteristic & CR-BE & PINN \\\\\\\\\\n\\\\midrule\\n\"\n    \n    # Use data for mesh size 64 (index 4) as reference point\n    mesh_64_idx = list(mesh_sizes).index(64) if 64 in mesh_sizes else -2  # Second to last as fallback\n    \n    eff_crbe_64 = df_crbe['rel_l2_error'].iloc[mesh_64_idx] * df_crbe['train_time'].iloc[mesh_64_idx]\n    eff_pinn_64 = df_pinn['rel_l2_error'].iloc[mesh_64_idx] * df_pinn['train_time'].iloc[mesh_64_idx]\n    \n    table6 += f\"Accuracy (Best $L^2$ Error) & {min_l2_crbe} & {min_l2_pinn} \\\\\\\\\\n\"\n    table6 += f\"Computational Efficiency (Time for mesh=64) & ${df_crbe['train_time'].iloc[mesh_64_idx]:.2f}$ s & ${df_pinn['train_time'].iloc[mesh_64_idx]:.2f}$ s \\\\\\\\\\n\"\n    \n    if memory_data is not None:\n        table6 += f\"Memory Usage (MB for mesh=64) & ${memory_data['cr_memory_mb'].iloc[mesh_64_idx]:.2f}$ & ${memory_data['pinn_memory_mb'].iloc[mesh_64_idx]:.2f}$ \\\\\\\\\\n\"\n    else:\n        table6 += f\"Memory Usage (MB for mesh=64) & $-$ & $-$ \\\\\\\\\\n\"\n        \n    table6 += f\"Convergence Rate ($L^2$) & ${crbe_l2_rate:.3f}$ & ${pinn_l2_rate:.3f}$ \\\\\\\\\\n\"\n    table6 += f\"Error/Cost Ratio (mesh=64) & ${eff_crbe_64:.2e}$ & ${eff_pinn_64:.2e}$ \\\\\\\\\\n\"\n    \n    table6 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    \n    # Table 7: Parameter sensitivity (if available)\n    table7 = \"\"\n    if sensitivity_data is not None:\n        table7 = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\"\n        table7 += \"\\\\caption{Sensitivity to diffusion coefficient}\\n\"\n        table7 += \"\\\\label{tab:parameter_sensitivity}\\n\"\n        table7 += \"\\\\begin{tabular}{cccccc}\\n\\\\toprule\\n\"\n        table7 += \"\\\\multirow{2}{*}{Diffusion ($D$)} & \\\\multirow{2}{*}{Mesh Size} & \"\n        table7 += \"\\\\multicolumn{2}{c}{$L^2$ Error} & \\\\multicolumn{2}{c}{Training Time (s)} \\\\\\\\\\n\"\n        table7 += \"\\\\cmidrule(lr){3-4} \\\\cmidrule(lr){5-6}\\n\"\n        table7 += \"& & CR-BE & PINN & CR-BE & PINN \\\\\\\\\\n\\\\midrule\\n\"\n        \n        # For each diffusion value and selected mesh sizes\n        diffusion_values = sensitivity_data['diffusion_coef'].unique()\n        mesh_selection = [32]  # Example: focus on mesh size 32\n        \n        for d in diffusion_values:\n            for mesh in mesh_selection:\n                df_d_mesh_cr = sensitivity_data[(sensitivity_data['diffusion_coef'] == d) & \n                                             (sensitivity_data['mesh_size'] == mesh) &\n                                             (sensitivity_data['method'] == 'CR-BE')]\n                \n                df_d_mesh_pinn = sensitivity_data[(sensitivity_data['diffusion_coef'] == d) & \n                                               (sensitivity_data['mesh_size'] == mesh) &\n                                               (sensitivity_data['method'] == 'PINN')]\n                \n                if len(df_d_mesh_cr) > 0 and len(df_d_mesh_pinn) > 0:\n                    cr_l2 = format_sci(df_d_mesh_cr['rel_l2_error'].iloc[0])\n                    pinn_l2 = format_sci(df_d_mesh_pinn['rel_l2_error'].iloc[0])\n                    cr_time = f\"${df_d_mesh_cr['train_time'].iloc[0]:.2f}$\"\n                    pinn_time = f\"${df_d_mesh_pinn['train_time'].iloc[0]:.2f}$\"\n                    \n                    table7 += f\"{d} & {mesh} & {cr_l2} & {pinn_l2} & {cr_time} & {pinn_time} \\\\\\\\\\n\"\n        \n        table7 += \"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\"\n    \n    tables = {\n        \"convergence_comparison\": table1,\n        \"convergence_rates\": table2,\n        \"computational_resources\": table3,\n        \"efficiency_comparison\": table4,\n        \"summary_statistics\": table5,\n        \"method_characteristics\": table6\n    }\n    \n    if table7:\n        tables[\"parameter_sensitivity\"] = table7\n        \n    return tables\n\n# Example usage\n# Generate tables from your results\ntables = generate_latex_tables(df_crbe, df_pinn)\n    \n# Write tables to file\nwith open('convergence_tables_2.tex', 'w') as f:\n    for name, table in tables.items():\n        f.write(f\"% {name}\\n\")\n        f.write(table)\n        f.write(\"\\n\\n\")\n            \nprint(\"LaTeX tables generated and saved to convergence_tables_2.tex\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd ..","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tar -czvf AirPollution.tar.gz AirPollution","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd AirPollution","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-29T14:49:46.745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}